{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "PiyjEOM1XoL7",
      "metadata": {
        "id": "PiyjEOM1XoL7"
      },
      "source": [
        "**IU000128 Coding3: Exploring Machine Intelligence Part 2** \n",
        "\n",
        "For more info, please check [Coding3 part 1](https://colab.research.google.com/drive/1NPteSsCJ89l697_ztAmcLCU_pGGjzvi5?usp=sharing)\n",
        "\n",
        "--- \n",
        "\n",
        "## Char-RNN and Text Generation - by YIFAN FENG\n",
        "\n",
        "\n",
        "#### Main Reference\n",
        "*   [Char-RNN Blog](https://geeks-today.medium.com/rnn-character-level-text-generation-with-tensorflow-2-0-from-scratch-to-deep-insights-41bac0e07f86)\n",
        "*   [Char-RNN Project: Trump-like Tweets](https://github.com/jctestud/char-rnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libs and Set up Env"
      ],
      "metadata": {
        "id": "mztFfrVMf1SC"
      },
      "id": "mztFfrVMf1SC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2VpHqlMwiZXv",
      "metadata": {
        "id": "2VpHqlMwiZXv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "226570f0-45b4-4abe-957f-b576c4a0926a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive #Access GoogleDrive \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7820857-0f2b-4cba-9034-33952a0c4fcd",
      "metadata": {
        "id": "d7820857-0f2b-4cba-9034-33952a0c4fcd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Processing: from Word to Character"
      ],
      "metadata": {
        "id": "ms2LiPoTcPgD"
      },
      "id": "ms2LiPoTcPgD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c71ae2dd",
      "metadata": {
        "id": "c71ae2dd"
      },
      "outputs": [],
      "source": [
        "#Import textfile generated from PART_ONE\n",
        "#text = open('./oxford_ai_cleaned.txt', 'rb').read().decode(encoding='utf-8') \n",
        "text = open('/content/drive/MyDrive/Coding3/oxford_ai_update.txt', 'rb').read().decode(encoding='utf-8') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bff4540-c2fe-4354-afb5-7665e3cf7922",
      "metadata": {
        "id": "4bff4540-c2fe-4354-afb5-7665e3cf7922"
      },
      "outputs": [],
      "source": [
        "#Word Embedding\n",
        "\n",
        "vocab = sorted(set(text)) \n",
        "#print('Length of text: {} characters.'.format(len(text)))\n",
        "char2idx = {char:i for i, char in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_encoded = [char2idx[c] for c in text]\n",
        "text_encoded = np.array(text_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cacd5087",
      "metadata": {
        "id": "cacd5087"
      },
      "outputs": [],
      "source": [
        "seq_length = 100 #set the length sentence for one input \n",
        "example_per_epoch = len(text)//seq_length # one example of seq_length characters.\n",
        "\n",
        "# Create training examples / targets \n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_encoded) #Decompose to character level\n",
        "sequences = char_dataset.batch(batch_size=seq_length+1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e430aa9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "0e430aa9",
        "outputId": "b43e21d5-0967-4600-d651-ef1358f5593b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor input_ex, target_ex in dataset.take(1):\\n    print('Input data: ', repr(''.join(idx2char[input_ex.numpy()])))\\n    print('Output data:',repr(''.join(idx2char[target_ex.numpy()])))\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "dataset = sequences.map(split_input_target)\n",
        "'''\n",
        "for input_ex, target_ex in dataset.take(1):\n",
        "    print('Input data: ', repr(''.join(idx2char[input_ex.numpy()])))\n",
        "    print('Output data:',repr(''.join(idx2char[target_ex.numpy()])))\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s9Gg-cyg4NWx",
      "metadata": {
        "id": "s9Gg-cyg4NWx"
      },
      "source": [
        "### Reconstruct Char-RNN Model \n",
        "\n",
        "\n",
        "*   [Char-RNN Source Code](https://github.com/sherjilozair/char-rnn-tensorflow)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "952db308",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "952db308",
        "outputId": "fd56c134-646d-4492-fca7-cb64d2984f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c31b3ca9",
      "metadata": {
        "id": "c31b3ca9"
      },
      "outputs": [],
      "source": [
        "#??dataset.shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87917b21",
      "metadata": {
        "id": "87917b21"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 512 #optional: 256\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                  output_dim = embedding_dim,\n",
        "                                  batch_input_shape = [batch_size, None]),\n",
        "\n",
        "        tf.keras.layers.GRU(units = 64,\n",
        "                            return_sequences= True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform',\n",
        "                            dropout = 0.1), \n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "        tf.keras.layers.GRU(units = 128,\n",
        "                            return_sequences= True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform',\n",
        "                            dropout = 0.1), \n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "        tf.keras.layers.GRU(units = 256,\n",
        "                            return_sequences= True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform',\n",
        "                            dropout = 0.1), \n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "         tf.keras.layers.GRU(units = 512,\n",
        "                            return_sequences= True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform',\n",
        "                            dropout = 0.1), \n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d66f938",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d66f938",
        "outputId": "f6c3f0b3-d5f0-4b22-9818-b201dd831c26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 512)           179200    \n",
            "                                                                 \n",
            " gru (GRU)                   (64, None, 64)            110976    \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (64, None, 64)           256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dropout (Dropout)           (64, None, 64)            0         \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (64, None, 128)           74496     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (64, None, 128)          512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (64, None, 128)           0         \n",
            "                                                                 \n",
            " gru_2 (GRU)                 (64, None, 256)           296448    \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (64, None, 256)          1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (64, None, 256)           0         \n",
            "                                                                 \n",
            " gru_3 (GRU)                 (64, None, 512)           1182720   \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (64, None, 512)          2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (64, None, 512)           0         \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 350)           179550    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,027,230\n",
            "Trainable params: 2,025,310\n",
            "Non-trainable params: 1,920\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = build_model(vocab_size = len(vocab),\n",
        "                    embedding_dim = embedding_dim,\n",
        "                    batch_size = BATCH_SIZE)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c11e12b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c11e12b",
        "outputId": "348277b7-e70f-41c9-dab7-5faf30fa33a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100, 350) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "# First check the shape of the output:\n",
        "for input_ex_batch, target_ex_batch in dataset.take(1):\n",
        "    ex_batch_prediction = model(input_ex_batch) # simply it takes input and calculate output with initial weights.\n",
        "    print(ex_batch_prediction.shape, \"# (batch_size, sequence_length, vocab_size)\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff7e03b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bff7e03b",
        "outputId": "8b22690b-eeb0-421d-da34-ac84efd2c070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 97  58 277   1 175 111 290 304 121 274 235 175 205 220 312 236  65 219\n",
            " 106 336 203   8  96 156 195  99  54 244  78 209 206 104 335  88 135 158\n",
            " 186 223  31 284 120 260 207 123 267 301   8 103 209  55  18 298 116 132\n",
            "  33 273  74  18 124 336 328 182 241 341 327 116 298  60  10  95  36 325\n",
            " 309   3 105  84  19  28 344 324  50 205 174 120 323 122 332 269 263 143\n",
            "  46 252 122 317 311 239 235 283 312 233]\n"
          ]
        }
      ],
      "source": [
        "sampled_indices = tf.random.categorical(ex_batch_prediction[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "#print(sampled_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "137048d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "137048d3",
        "outputId": "1c868157-ff65-4d85-867a-c8bd5f3e35a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 350)  # (batch_size, sequence_length, vocab_size)\n",
            "Scaler loss:  5.858283\n"
          ]
        }
      ],
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "ex_batch_loss = loss(target_ex_batch, ex_batch_prediction)\n",
        "#print(\"Prediction shape: \", ex_batch_prediction.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "#print(\"Scaler loss: \", ex_batch_loss.numpy().mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "448b2cde",
      "metadata": {
        "id": "448b2cde"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86490f0a",
      "metadata": {
        "id": "86490f0a",
        "outputId": "c87a5b93-5c3e-41e7-ffc2-b58a3514f331",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 2.7453\n",
            "Epoch 2/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.8635\n",
            "Epoch 3/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.7135\n",
            "Epoch 4/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.6371\n",
            "Epoch 5/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.5903\n",
            "Epoch 6/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.5586\n",
            "Epoch 7/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.5350\n",
            "Epoch 8/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.5181\n",
            "Epoch 9/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.5036\n",
            "Epoch 10/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.4912\n",
            "Epoch 11/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.4816\n",
            "Epoch 12/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.4729\n",
            "Epoch 13/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.4654\n",
            "Epoch 14/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.4578\n",
            "Epoch 15/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.4523\n",
            "Epoch 16/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.4476\n",
            "Epoch 17/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.4427 1s\n",
            "Epoch 18/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.4387\n",
            "Epoch 19/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.4333\n",
            "Epoch 20/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.4294\n",
            "Epoch 21/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.4271\n",
            "Epoch 22/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.4242 1\n",
            "Epoch 23/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.4212\n",
            "Epoch 24/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.4184\n",
            "Epoch 25/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.4152\n",
            "Epoch 26/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.4136\n",
            "Epoch 27/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.4112\n",
            "Epoch 28/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.4088\n",
            "Epoch 29/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.4065\n",
            "Epoch 30/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.4040\n",
            "Epoch 31/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.4031\n",
            "Epoch 32/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.4015\n",
            "Epoch 33/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3991\n",
            "Epoch 34/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3977\n",
            "Epoch 35/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3964: 0s - loss: 1.396\n",
            "Epoch 36/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3949\n",
            "Epoch 37/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3936\n",
            "Epoch 38/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3926\n",
            "Epoch 39/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3911\n",
            "Epoch 40/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3902\n",
            "Epoch 41/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3892\n",
            "Epoch 42/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3878 0s - loss\n",
            "Epoch 43/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3864\n",
            "Epoch 44/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3854\n",
            "Epoch 45/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3837\n",
            "Epoch 46/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3841\n",
            "Epoch 47/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3828 1 - ETA: 0s - loss: 1\n",
            "Epoch 48/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3817\n",
            "Epoch 49/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3806\n",
            "Epoch 50/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3797 8s - - ET\n",
            "Epoch 51/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3795\n",
            "Epoch 52/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3783\n",
            "Epoch 53/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3780\n",
            "Epoch 54/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3766\n",
            "Epoch 55/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3757\n",
            "Epoch 56/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3751\n",
            "Epoch 57/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3745\n",
            "Epoch 58/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3739\n",
            "Epoch 59/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3740\n",
            "Epoch 60/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3722\n",
            "Epoch 61/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3723\n",
            "Epoch 62/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3711\n",
            "Epoch 63/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3704\n",
            "Epoch 64/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3702\n",
            "Epoch 65/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3695 1\n",
            "Epoch 66/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3687\n",
            "Epoch 67/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3687\n",
            "Epoch 68/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3677\n",
            "Epoch 69/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3674 0s - loss: 1\n",
            "Epoch 70/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3672\n",
            "Epoch 71/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3664\n",
            "Epoch 72/500\n",
            "290/290 [==============================] - 10s 34ms/step - loss: 1.3654\n",
            "Epoch 73/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3655\n",
            "Epoch 74/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3654\n",
            "Epoch 75/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3640\n",
            "Epoch 76/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3640\n",
            "Epoch 77/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3632 0s - lo\n",
            "Epoch 78/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3635 0s - loss: \n",
            "Epoch 79/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3628 5s - l - ET - E\n",
            "Epoch 80/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3623\n",
            "Epoch 81/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3608\n",
            "Epoch 82/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3613\n",
            "Epoch 83/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3604\n",
            "Epoch 84/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3616\n",
            "Epoch 85/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3606\n",
            "Epoch 86/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3602\n",
            "Epoch 87/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3595\n",
            "Epoch 88/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3591\n",
            "Epoch 89/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3593\n",
            "Epoch 90/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3594\n",
            "Epoch 91/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3581\n",
            "Epoch 92/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3584\n",
            "Epoch 93/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3567\n",
            "Epoch 94/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3583\n",
            "Epoch 95/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3588\n",
            "Epoch 96/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3566\n",
            "Epoch 97/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3567\n",
            "Epoch 98/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3566\n",
            "Epoch 99/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3557\n",
            "Epoch 100/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3559\n",
            "Epoch 101/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3554\n",
            "Epoch 102/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3554\n",
            "Epoch 103/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3551\n",
            "Epoch 104/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3546 0s - loss: 1.3 - ETA: 0s - loss: 1\n",
            "Epoch 105/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3541\n",
            "Epoch 106/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3542\n",
            "Epoch 107/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3540\n",
            "Epoch 108/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3541\n",
            "Epoch 109/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3533\n",
            "Epoch 110/500\n",
            "290/290 [==============================] - 11s 39ms/step - loss: 1.3532\n",
            "Epoch 111/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3532\n",
            "Epoch 112/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3528 1\n",
            "Epoch 113/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3523\n",
            "Epoch 114/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3520\n",
            "Epoch 115/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3516 0s - loss: 1\n",
            "Epoch 116/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3511 1\n",
            "Epoch 117/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3519\n",
            "Epoch 118/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3516\n",
            "Epoch 119/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3512\n",
            "Epoch 120/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3508 0s - \n",
            "Epoch 121/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3516\n",
            "Epoch 122/500\n",
            "290/290 [==============================] - 12s 40ms/step - loss: 1.3509\n",
            "Epoch 123/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3501 0s - loss: 1.350\n",
            "Epoch 124/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3504 8s - - E - ETA: 0s - loss: 1.350\n",
            "Epoch 125/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3509\n",
            "Epoch 126/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3501\n",
            "Epoch 127/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3494\n",
            "Epoch 128/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3497\n",
            "Epoch 129/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3497\n",
            "Epoch 130/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3493\n",
            "Epoch 131/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3492\n",
            "Epoch 132/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3487\n",
            "Epoch 133/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3490\n",
            "Epoch 134/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3479\n",
            "Epoch 135/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3492\n",
            "Epoch 136/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3478\n",
            "Epoch 137/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3486\n",
            "Epoch 138/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3480\n",
            "Epoch 139/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3478\n",
            "Epoch 140/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3478 0s - loss: 1\n",
            "Epoch 141/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3488\n",
            "Epoch 142/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3474\n",
            "Epoch 143/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3473\n",
            "Epoch 144/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3469\n",
            "Epoch 145/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3463\n",
            "Epoch 146/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3467\n",
            "Epoch 147/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3470\n",
            "Epoch 148/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3461\n",
            "Epoch 149/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3457\n",
            "Epoch 150/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3456\n",
            "Epoch 151/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3462 1s\n",
            "Epoch 152/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3457 0s - loss: 1\n",
            "Epoch 153/500\n",
            "290/290 [==============================] - 11s 39ms/step - loss: 1.3469\n",
            "Epoch 154/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3463\n",
            "Epoch 155/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3455\n",
            "Epoch 156/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3456\n",
            "Epoch 157/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3455\n",
            "Epoch 158/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3460\n",
            "Epoch 159/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3447 1s\n",
            "Epoch 160/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3452 ETA: 2s - lo\n",
            "Epoch 161/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3463\n",
            "Epoch 162/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3456\n",
            "Epoch 163/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3451\n",
            "Epoch 164/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3451\n",
            "Epoch 165/500\n",
            "290/290 [==============================] - 11s 39ms/step - loss: 1.3442\n",
            "Epoch 166/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3452\n",
            "Epoch 167/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3458\n",
            "Epoch 168/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3446\n",
            "Epoch 169/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3433\n",
            "Epoch 170/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3438\n",
            "Epoch 171/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3450\n",
            "Epoch 172/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3441\n",
            "Epoch 173/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3446\n",
            "Epoch 174/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3455 0s - lo\n",
            "Epoch 175/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3439 1s\n",
            "Epoch 176/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3440\n",
            "Epoch 177/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3434 1s - loss: 1.342\n",
            "Epoch 178/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3498 1s - loss: 1. - ETA: 1\n",
            "Epoch 179/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3474\n",
            "Epoch 180/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3433\n",
            "Epoch 181/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3428\n",
            "Epoch 182/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3426\n",
            "Epoch 183/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3438\n",
            "Epoch 184/500\n",
            "290/290 [==============================] - 11s 39ms/step - loss: 1.3430 0s - loss: 1\n",
            "Epoch 185/500\n",
            "290/290 [==============================] - 12s 40ms/step - loss: 1.3469\n",
            "Epoch 186/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3446\n",
            "Epoch 187/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3433\n",
            "Epoch 188/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3436- ETA: 0s - loss: 1 - ETA: 0s - loss: 1.343\n",
            "Epoch 189/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3443 E\n",
            "Epoch 190/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3427\n",
            "Epoch 191/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3416\n",
            "Epoch 192/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3424\n",
            "Epoch 193/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3418\n",
            "Epoch 194/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3447\n",
            "Epoch 195/500\n",
            "290/290 [==============================] - 11s 39ms/step - loss: 1.3419\n",
            "Epoch 196/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3418\n",
            "Epoch 197/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3428\n",
            "Epoch 198/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3411\n",
            "Epoch 199/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3415\n",
            "Epoch 200/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3428 \n",
            "Epoch 201/500\n",
            "290/290 [==============================] - 12s 40ms/step - loss: 1.3434\n",
            "Epoch 202/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3439\n",
            "Epoch 203/500\n",
            "290/290 [==============================] - 11s 39ms/step - loss: 1.3425\n",
            "Epoch 204/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3593\n",
            "Epoch 205/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3453\n",
            "Epoch 206/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3436\n",
            "Epoch 207/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3585\n",
            "Epoch 208/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3490\n",
            "Epoch 209/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3424\n",
            "Epoch 210/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3427\n",
            "Epoch 211/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3402\n",
            "Epoch 212/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3411\n",
            "Epoch 213/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3404\n",
            "Epoch 214/500\n",
            "290/290 [==============================] - 10s 34ms/step - loss: 1.3410\n",
            "Epoch 215/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3428\n",
            "Epoch 216/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3405\n",
            "Epoch 217/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3422\n",
            "Epoch 218/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3703\n",
            "Epoch 219/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3495\n",
            "Epoch 220/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3438\n",
            "Epoch 221/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3424\n",
            "Epoch 222/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3405\n",
            "Epoch 223/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3414\n",
            "Epoch 224/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3405\n",
            "Epoch 225/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3423\n",
            "Epoch 226/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3430\n",
            "Epoch 227/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3412\n",
            "Epoch 228/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3421\n",
            "Epoch 229/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3405\n",
            "Epoch 230/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3414\n",
            "Epoch 231/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3430\n",
            "Epoch 232/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3431 0s - loss: 1 - ETA: 0s - loss: \n",
            "Epoch 233/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3442 E\n",
            "Epoch 234/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3431 6 - ETA: 1s - loss: 1.3 - \n",
            "Epoch 235/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3459\n",
            "Epoch 236/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3485\n",
            "Epoch 237/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3495\n",
            "Epoch 238/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3442\n",
            "Epoch 239/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3462\n",
            "Epoch 240/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3424\n",
            "Epoch 241/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3422\n",
            "Epoch 242/500\n",
            "290/290 [==============================] - 11s 39ms/step - loss: 1.3420\n",
            "Epoch 243/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3445\n",
            "Epoch 244/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3423\n",
            "Epoch 245/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3738\n",
            "Epoch 246/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3581\n",
            "Epoch 247/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3501\n",
            "Epoch 248/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3523\n",
            "Epoch 249/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3465\n",
            "Epoch 250/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3632\n",
            "Epoch 251/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3553\n",
            "Epoch 252/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3495\n",
            "Epoch 253/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3457\n",
            "Epoch 254/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3504\n",
            "Epoch 255/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3447\n",
            "Epoch 256/500\n",
            "290/290 [==============================] - 10s 35ms/step - loss: 1.3423\n",
            "Epoch 257/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3439\n",
            "Epoch 258/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3897\n",
            "Epoch 259/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3923\n",
            "Epoch 260/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3817\n",
            "Epoch 261/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3675\n",
            "Epoch 262/500\n",
            "290/290 [==============================] - 11s 39ms/step - loss: 1.3627 8s - loss: 1\n",
            "Epoch 263/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3573\n",
            "Epoch 264/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.3561\n",
            "Epoch 265/500\n",
            "290/290 [==============================] - 11s 39ms/step - loss: 1.3566\n",
            "Epoch 266/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3539 5s - ETA: 0s - loss: 1.353\n",
            "Epoch 267/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3484\n",
            "Epoch 268/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3502\n",
            "Epoch 269/500\n",
            "290/290 [==============================] - 11s 38ms/step - loss: 1.3507\n",
            "Epoch 270/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3528\n",
            "Epoch 271/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3532\n",
            "Epoch 272/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3495\n",
            "Epoch 273/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3465\n",
            "Epoch 274/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3541\n",
            "Epoch 275/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3539\n",
            "Epoch 276/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3580\n",
            "Epoch 277/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.3552\n",
            "Epoch 278/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.3497\n",
            "Epoch 279/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.4111\n",
            "Epoch 280/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.6147\n",
            "Epoch 281/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.5962\n",
            "Epoch 282/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.4925\n",
            "Epoch 283/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.4704\n",
            "Epoch 284/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.4486\n",
            "Epoch 285/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.4135\n",
            "Epoch 286/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.4105\n",
            "Epoch 287/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 1.4013\n",
            "Epoch 288/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 1.4062\n",
            "Epoch 289/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 1.9467\n",
            "Epoch 290/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 2.4475\n",
            "Epoch 291/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 2.4063\n",
            "Epoch 292/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 2.4514\n",
            "Epoch 293/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 2.4041\n",
            "Epoch 294/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 2.3423\n",
            "Epoch 295/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 2.2907\n",
            "Epoch 296/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 2.2446\n",
            "Epoch 297/500\n",
            "290/290 [==============================] - 10s 36ms/step - loss: 2.2037\n",
            "Epoch 298/500\n",
            "290/290 [==============================] - 11s 36ms/step - loss: 2.1698\n",
            "Epoch 299/500\n",
            "290/290 [==============================] - 11s 37ms/step - loss: 2.1171\n",
            "Epoch 300/500\n",
            " 84/290 [=======>......................] - ETA: 7s - loss: 2.0660"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19440/263454488.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcheckpoint_callback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37-tf23-cuda10.1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37-tf23-cuda10.1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37-tf23-cuda10.1\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37-tf23-cuda10.1\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37-tf23-cuda10.1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37-tf23-cuda10.1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37-tf23-cuda10.1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37-tf23-cuda10.1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py37-tf23-cuda10.1\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = 'C:/Users/21036265/Desktop/charrnn_checkpoints'\n",
        "\n",
        "# Checkpoint file only save training weight. Save after 10 epochs\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True, save_freq=10)\n",
        "\n",
        "EPOCHS = 500 #early stop at 300 due to loss increase\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84e6012d",
      "metadata": {
        "id": "84e6012d"
      },
      "outputs": [],
      "source": [
        "#Restore training weights \n",
        "model = build_model(vocab_size, embedding_dim, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir), by_name=False ,skip_mismatch=False )\n",
        "\n",
        "# Builds the model based on input shapes received.\n",
        "model.build(tf.TensorShape([1, None])) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O0h_vlfS4AXD",
      "metadata": {
        "id": "O0h_vlfS4AXD"
      },
      "source": [
        "### Generate Some Conditional Text!!!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f62496f4",
      "metadata": {
        "id": "f62496f4"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_string, num_generate, temperature):\n",
        "    \n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx [s] for s in start_string.lower()]\n",
        "    # convert (x,y) shaped matrix to (1,x,y).\n",
        "    input_eval = tf.expand_dims(input_eval, axis=0) \n",
        "    \n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "    \n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        \n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        \n",
        "        # using a categorical distribution to predict the \n",
        "        # character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        \n",
        "        # We got the predictions for every timestep but we \n",
        "        # want only last so first we take [-1] to consider on last \n",
        "        # predictions distribution only and after we try to get id \n",
        "        # from 1D array. Ex. we got '2' from a=['2'] by a[0].\n",
        "        predicted_id = tf.random.categorical(predictions, \n",
        "                                             num_samples=1\n",
        "                                            )[-1,0].numpy()\n",
        "        \n",
        "        # We pass the predicted character as the next input to the \n",
        "        # model along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        \n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "        \n",
        "    return (start_string + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ade312fb",
      "metadata": {
        "id": "ade312fb",
        "outputId": "c881d5a7-56db-473e-8ab8-086c923619a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Responsible AI issues sections ai systems section algorith constion algorith conter algorithm simple algorithmation algorith develop alson conter artificial system accounted comple algorith algorither algorither ai sy'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model,start_string=u'Responsible AI is',num_generate=200,temperature=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01936564",
      "metadata": {
        "id": "01936564",
        "outputId": "54f7b883-bcff-4ed8-cae5-419bda591954"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Artificial Intelligence issuts procent proper comple stated comple ai sect ai see contion accounted algorithment proces social alson procention complecing ai system sections stantical interneted comple human procenting access '"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model,start_string=u'Artificial Intelligence is',num_generate=200,temperature=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1f6f059",
      "metadata": {
        "id": "c1f6f059",
        "outputId": "9c0053d0-7a4c-45cd-866f-75852ca5b310"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Ethical AI issibility scountificial conter accounted comple secter may interneter sections stantificial system human respons artificial setal result internations entificial ai ponted sect ai alson section proces c'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model,start_string=u'Ethical AI is',num_generate=200,temperature=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d46e4c8",
      "metadata": {
        "id": "1d46e4c8",
        "outputId": "9cdb7e9d-9aca-4996-aa55-d2a7c84167e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Accountability issues ai algorither conteration ai social systems sections respect algorith algority comple state section conter process ai systems conseques ai proce conter artificial respect ai systems conseques acc'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model,start_string=u'Accountability is',num_generate=200,temperature=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b55b70ca",
      "metadata": {
        "id": "b55b70ca",
        "outputId": "5c772f8a-9bc0-4530-867d-bbfce837c8fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Equitable AI issues secte conter algorithm conter artificial systems sections sections ai systems section algorith constical ai systems sections sections proven ai stantificial section conter algorither algorither a'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model,start_string=u'Equitable AI is',num_generate=200,temperature=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d49c4729",
      "metadata": {
        "id": "d49c4729",
        "outputId": "cd523bfa-514d-4c2e-9cd4-b0a1901bba16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Accountable AI issess sections ai system constions designing advertion prove provers design contion algorith section experted ai accounted comple algorither sections experent algority sections sections ai sect ai cont'"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model,start_string=u'Accountable AI is',num_generate=200,temperature=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a7a5a04",
      "metadata": {
        "id": "8a7a5a04",
        "outputId": "99ddb765-fc90-4339-8c12-40f4bac3048d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Responsibility issues section algorithms secte algorithm conteration experting ai system ai sections stantern algority algorithm sections ai system constions sections ai systems sections accounted conter algorith cons'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model,start_string=u'Responsibility is',num_generate=200,temperature=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29fcc406",
      "metadata": {
        "id": "29fcc406",
        "outputId": "402c1edc-d915-4aa0-fed1-f32bcd442228"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Equity istion action algorith also contertions ai systems section conter algorith consition secte artificial interneted ai systems sections ai systems section algorith constion algorith constitute section ai s'"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model,start_string=u'Equity is',num_generate=200,temperature=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c09bc61b",
      "metadata": {
        "id": "c09bc61b",
        "outputId": "4bf1fc4d-372c-4cd5-c8fc-eac8383f1a93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Ethics issues sections ai systems comple algorithm sections ai stantificial intelligence sected contion algorith consticul antical respons artificial siment algorithm conter artificial systems conseques ai art'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model,start_string=u'Ethics is',num_generate=200,temperature=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd1e792",
      "metadata": {
        "id": "bbd1e792"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Coding_three_part_two_update.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mztFfrVMf1SC"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}